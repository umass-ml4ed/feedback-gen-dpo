from typing import Optional
import argparse
import os
from itertools import combinations
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import TrainingArguments, Trainer, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from trl import PPOTrainer, PPOConfig, DPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset as HFDataset
from tqdm import tqdm
import wandb

from reward_model import get_check_correct_batch_fn
from reward_model_dataset import load_dataset as load_annotated_dataset, LABELS
from create_reward_dataset import FEEDBACK_GEN_INSTRUCTION, FEEDBACK_GEN_INSTRUCTION_RUBRIC
from utils import initialize_seeds, device, bool_type




######## MODEL ########

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

peft_config = LoraConfig(
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    r=32,
    lora_alpha=16,
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
    inference_mode=False,
)

def get_base_model(base_model_name: str, tokenizer: AutoTokenizer):
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        pad_token_id=tokenizer.pad_token_id,
        quantization_config=bnb_config,
        torch_dtype=torch.float32,
        device_map={"": 0}
    )
    base_model.config.use_cache = False
    base_model.config.pretraining_tp = 1
    return base_model

def get_model(base_model_name: str, model_name: Optional[str], pt_model_name: Optional[str],
              include_value_head: bool, test: bool, use_gradient_checkpointing: bool = True):
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    model = get_base_model(base_model_name, tokenizer)
    if test and model_name:
        # Note we are loading adapter on quantized model and not merging
        # Recommended here - https://huggingface.co/docs/trl/main/en/dpo_trainer#downsides-to-merging-qlora-before-dpo-approach-2
        # Also prevents empty responses generated by Llama models
        model = PeftModel.from_pretrained(model, model_name)
    else:
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)
        if pt_model_name:
            model = PeftModel.from_pretrained(model, pt_model_name, is_trainable=True, adapter_name="default")
            # Load same lora weights to separate adapter to be used as reference model
            model.load_adapter(pt_model_name, adapter_name="lora_ref")
        else:
            model = get_peft_model(model, peft_config)
        # Wrap model to add (newly initialized) value head for PPO training
        if include_value_head:
            model = AutoModelForCausalLMWithValueHead(model).to(device)
            model.is_peft_model = True # Tells PPO trainer to disable adapters to recover reference model
    return model, tokenizer




######## DATA ########

def load_dataset(split: str):
    return pd.read_csv(f"data/raw/eedi_expanded_{split}.csv")

def clean(text):
    return str(text).strip().replace("\n", " ")

def get_prompt(row: pd.Series, include_rubric: bool):
    return "You are a math education expert.\n" +\
        f"{FEEDBACK_GEN_INSTRUCTION}\n" +\
        (FEEDBACK_GEN_INSTRUCTION_RUBRIC + "\n" if include_rubric else "") +\
        f"\nQuestion: {clean(row['question'])}\n" +\
        f"Correct Answer: {clean(row['correct_answer'])}\n" +\
        f"Solution: {clean(row['explanation'])}\n" +\
        f"Student's Incorrect Answer: {clean(row['distractor'])}\n" +\
        "Feedback:"

def get_completion(row: pd.Series):
    return str(row["feedback"]).strip()

class FeedbackSFTDataset(Dataset):
    def __init__(self, data: pd.DataFrame, args):
        self.data = [
            {
                **row.to_dict(),
                "prompt": get_prompt(row, args.include_rubric),
                "label": get_completion(row),
            }
            for _, row in data.iterrows()
        ]

    def __getitem__(self, index: int):
        return self.data[index]

    def __len__(self):
        return len(self.data)

class FeedbackSFTCollator:
    def __init__(self, tokenizer, test: bool):
        self.tokenizer = tokenizer
        self.test = test

    def __call__(self, batch):
        all_prompts = [sample["prompt"] for sample in batch]
        prompts_tokenized = self.tokenizer(all_prompts, return_tensors="pt", padding=True)
        if self.test:
            return {
                "input_ids": prompts_tokenized.input_ids.to(device),
                "attention_mask": prompts_tokenized.attention_mask.to(device),
                "meta_data": batch
            }

        all_inputs = [sample["prompt"] + sample["label"] + self.tokenizer.eos_token for sample in batch]
        inputs_tokenized = self.tokenizer(all_inputs, return_tensors="pt", padding=True)
        prompt_lens = prompts_tokenized.attention_mask.sum(dim=1)
        labels = inputs_tokenized.input_ids.clone()
        padding_mask = torch.arange(labels.shape[1]).repeat(labels.shape[0], 1) < prompt_lens.unsqueeze(1)
        labels[padding_mask] = -100
        labels = labels.masked_fill(inputs_tokenized.attention_mask == 0, -100)
        return {
            "input_ids": inputs_tokenized.input_ids,
            "attention_mask": inputs_tokenized.attention_mask,
            "labels": labels
        }

class FeedbackPPODataset(Dataset):
    def __init__(self, data: pd.DataFrame, tokenizer: AutoTokenizer, args):
        self.data = []
        for _, row in data.iterrows():
            prompt = get_prompt(row, args.include_rubric),
            self.data.append({
                "query": prompt,
                "input_ids": tokenizer.encode(prompt, return_tensors="pt")[0],
                "meta_data": row.to_dict()
            })

    def __getitem__(self, index: int):
        return self.data[index]

    def __len__(self):
        return len(self.data)

class FeedbackPPOCollator:
    def __call__(self, batch):
        # PPO trainer methods expect list of tensors as input and handle padding internally
        return {
            "query": [sample["query"] for sample in batch],
            "input_ids": [sample["input_ids"] for sample in batch],
            "meta_data": [sample["meta_data"] for sample in batch]
        }

class FeedbackDPODataset(Dataset):
    def __init__(self, data: pd.DataFrame, src_data: pd.DataFrame, args):
        self.data = []

        # Pair with feedbacks mismatched from other questions
        for _ in range(args.dpo_mmo):
            mismatch_samples = src_data.copy()
            mismatch_samples = mismatch_samples.sample(frac=1, ignore_index=True)
            for (_, row), (_, alt_row) in zip(src_data.iterrows(), mismatch_samples.iterrows()):
                self.data.append({
                    "prompt": get_prompt(row, args.include_rubric),
                    "chosen": get_completion(row),
                    "rejected": get_completion(alt_row)
                })

        # Pair with feedbacks mismatched within same question
        if args.dpo_mmi:
            idxs = np.arange(len(src_data))
            for offset in range(1, 3):
                mismatch_samples = src_data.copy()
                target_idxs = idxs - idxs % 3 + (idxs + offset) % 3
                mismatch_samples["feedback"] = src_data.iloc[target_idxs]["feedback"].values
                for (_, row), (_, alt_row) in zip(src_data.iterrows(), mismatch_samples.iterrows()):
                    if row["feedback"] == alt_row["feedback"]:
                        continue # Can't create preference pair when feedback is the same
                    self.data.append({
                        "prompt": get_prompt(row, args.include_rubric),
                        "chosen": get_completion(row),
                        "rejected": get_completion(alt_row)
                    })

        # Pair annotated feedbacks within distractor, select winner based on score
        if args.dpo_score:
            data["incorrect"] = 1 - data["incorrect"]
            data["reveal"] = 1 - data["reveal"]
            data["score"] = data[LABELS[0]] * sum([data[label] for label in LABELS]) / len(LABELS)
            grouped_data = data.groupby(["question", "distractor"])
            for _, group in grouped_data:
                if len(group) == 1:
                    continue # No other feedbacks to pair with
                for (_, row0), (_, row1) in combinations(group.iterrows(), 2):
                    if row0["score"] == row1["score"]:
                        continue # Neither feedback is preferred
                    elif row0["score"] > row1["score"]:
                        win_row, lose_row = row0, row1
                    else:
                        win_row, lose_row = row1, row0
                    self.data.append({
                        "prompt": get_prompt(row0, args.include_rubric),
                        "chosen": get_completion(win_row),
                        "rejected": get_completion(lose_row)
                    })

    def __getitem__(self, index: int):
        return self.data[index]

    def __len__(self):
        return len(self.data)




######## TRAINING ########

def get_training_args(args):
    return TrainingArguments(
        output_dir=args.model_name,
        num_train_epochs=args.epochs,
        learning_rate=args.lr,
        weight_decay=args.wd,
        max_grad_norm=args.max_grad_norm or None,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum_steps,
        per_device_eval_batch_size=args.batch_size * 2,
        eval_accumulation_steps=4,
        warmup_ratio=0.1,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=1,
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        greater_is_better=False,
        remove_unused_columns=False,
        report_to="wandb" if args.wandb else "none"
    )

def sft(args):
    assert args.model_name
    model, tokenizer = get_model(args.base_model, None, None, False, False)
    trainer = Trainer(
        model=model,
        args=get_training_args(args),
        train_dataset=FeedbackSFTDataset(load_dataset("train"), args),
        eval_dataset=FeedbackSFTDataset(load_dataset("val"), args),
        data_collator=FeedbackSFTCollator(tokenizer, False)
    )
    trainer.train(resume_from_checkpoint=args.resume)
    trainer.save_model()

def ppo(args):
    assert args.model_name and args.pt_model_name and args.rm_name
    assert not args.resume, "Can't resume PPO training"

    model, tokenizer = get_model(args.base_model, None, args.pt_model_name, True, False)

    reward_fn = get_check_correct_batch_fn(args.rm_name, args.rm_base, False)

    train_dataset = FeedbackPPODataset(load_dataset("train"), tokenizer, args)
    val_dataset = FeedbackPPODataset(load_dataset("val"), tokenizer, args)
    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, collate_fn=FeedbackPPOCollator())

    config = PPOConfig(
        learning_rate=args.lr,
        batch_size=args.batch_size,
        mini_batch_size=args.mini_batch_size,
        gradient_accumulation_steps=args.grad_accum_steps,
        max_grad_norm=args.max_grad_norm,
        log_with="wandb" if args.wandb else None,
    )
    ppo_trainer = PPOTrainer(
        model=model,
        config=config,
        dataset=train_dataset,
        data_collator=FeedbackPPOCollator(),
        tokenizer=tokenizer,
    )

    # TODO: overwrite ppo_trainer.optional_peft_ctx to temporarily load reference adapter, similar to dpo_trainer

    train_generation_kwargs = {
        "do_sample": True,
        "top_k": 0,
        "top_p": 1.0,
        "pad_token_id": tokenizer.eos_token_id,
        "max_new_tokens": args.max_gen_tokens,
    }
    val_generation_kwargs = {
        "do_sample": False,
        "pad_token_id": tokenizer.eos_token_id,
        "max_new_tokens": args.max_gen_tokens,
    }

    best_val_reward = float("-inf")
    for _ in range(args.epochs):
        for batch in tqdm(ppo_trainer.dataloader):
            query_tensors = batch["input_ids"]

            # Get response from SFTModel
            # NOTE: if gradient checkpointing used, then generate causes
            #       "UserWarning: None of the inputs have requires_grad=True. Gradients will be None"
            #       but this seems safe to ignore
            response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **train_generation_kwargs)
            batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)

            # Compute reward score
            rewards = reward_fn(batch["meta_data"], batch["response"])
            rewards = [r for r in rewards] # Convert to list of tensors

            # Run PPO step
            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
            ppo_trainer.log_stats(stats, batch, rewards)

        total_val_reward = 0
        for batch in tqdm(val_dataloader):
            query_tensors = batch["input_ids"]

            # Get response from SFTModel
            response_tensors = ppo_trainer.generate(query_tensors, **val_generation_kwargs)
            batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

            # Compute reward score
            rewards = reward_fn(batch["meta_data"], batch["response"])
            total_val_reward += rewards.sum().item()

        avg_val_reward = total_val_reward / len(val_dataset)
        print(f"Val Reward: {avg_val_reward:.2f}")
        if args.wandb:
            wandb.log({"val_reward": avg_val_reward})

        if avg_val_reward > best_val_reward:
            print("Best! Saving model...")
            best_val_reward = avg_val_reward
            if not os.path.isdir(args.model_name):
                os.mkdir(args.model_name) # Needed for value head model
            model.save_pretrained(args.model_name)

def dpo(args):
    assert args.model_name and args.model_name != args.pt_model_name
    if not args.pt_model_name:
        print("Warning: no pre-trained model name provided, initializing to base model.")

    model, tokenizer = get_model(args.base_model, None, args.pt_model_name, False, False)

    train_dataset = FeedbackDPODataset(*load_annotated_dataset("train"), args)
    val_dataset = FeedbackDPODataset(*load_annotated_dataset("val"), args)

    trainer = DPOTrainer(
        model,
        model_adapter_name="default",
        ref_adapter_name="lora_ref" if args.pt_model_name else None,
        args=get_training_args(args),
        beta=args.beta,
        train_dataset=HFDataset.from_list(train_dataset.data),
        eval_dataset=HFDataset.from_list(val_dataset.data),
        tokenizer=tokenizer,
        generate_during_eval=False,
        precompute_ref_log_probs=True, # Avoid re-calculating reference model log probs every epoch
        max_length=2048,
        max_prompt_length=2048
    )
    # NOTE: will get "Could not estimate the number of tokens of the input, floating-point operations will not be computed"
    #       but not an issue since only used for logging
    trainer.train(resume_from_checkpoint=args.resume)
    trainer.save_model()

def generate(args):
    if not args.model_name:
        print("Warning: no model name provided, using base model.")
    model, tokenizer = get_model(args.base_model, args.model_name, args.pt_model_name, False, True)
    tokenizer.padding_side = "left"
    test_dataset = FeedbackSFTDataset(load_dataset("test"), args)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=FeedbackSFTCollator(tokenizer, True))
    generate_args = {"do_sample": False, "num_beams": args.num_beams} if args.decoding == "greedy" else {"do_sample": True, "top_p": 0.9, "temperature": 1.0}
    results = []
    for batch in tqdm(test_loader):
        output_ids = model.generate(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            pad_token_id=tokenizer.eos_token_id,
            max_new_tokens=args.max_gen_tokens,
            **generate_args
        )
        preds = tokenizer.batch_decode(output_ids[:, batch["input_ids"].shape[1]:], skip_special_tokens=True)
        preds = [pred.split("\n")[0] for pred in preds] # Base model generates artifacts after first line
        results += [{**sample, "prediction": pred.strip()} for sample, pred in zip(batch["meta_data"], preds)]
    results_df = pd.DataFrame(results)
    model_name = args.model_name or args.base_model + ("_rubric" if args.include_rubric else "")
    model_name = model_name.replace("/", "-")
    decoding = f"beam{args.num_beams}" if args.num_beams > 1 else args.decoding
    results_df.to_csv(f"feedback_gen_results_{model_name}_{decoding}.csv", index=False)

def main():
    initialize_seeds(221)

    parser = argparse.ArgumentParser()
    # Modes
    parser.add_argument("--sft", action="store_true", help="Supervised finetuning for feedback generation")
    parser.add_argument("--ppo", action="store_true", help="PPO training with reward model for feedback generation")
    parser.add_argument("--dpo", action="store_true", help="DPO training with GPT-4 annotations for feedback generation")
    parser.add_argument("--generate", action="store_true", help="Generate feedback with trained model")
    # Settings
    parser.add_argument("--base_model", type=str, default="meta-llama/Llama-2-7b-chat-hf", help="Pre-trained base model path")
    parser.add_argument("--model_name", type=str, help="Name of model to save for training or load for testing")
    parser.add_argument("--pt_model_name", type=str, help="Name of pre-trained (SFT) model for RL training")
    parser.add_argument("--rm_base", type=str, default="google/flan-t5-xl", help="Reward model base")
    parser.add_argument("--rm_name", type=str, help="Comma-separated reward model paths for reward model ensemble")
    parser.add_argument("--beta", type=float, default=0.5, help="KL regularization coefficient for DPO training")
    parser.add_argument("--dpo_score", type=bool_type, default=True, help="Include GPT-4 augmented data score comparisons for DPO training")
    parser.add_argument("--dpo_mmo", type=int, default=1, help="Mismatch outer rate for DPO training")
    parser.add_argument("--dpo_mmi", type=bool_type, default=True, help="Include inner mismatched feedbacks for DPO training")
    parser.add_argument("--resume", action="store_true", help="Resume training from checkpoint")
    parser.add_argument("--decoding", type=str, choices=["greedy", "sample"], default="greedy", help="Decoding strategy for generation")
    parser.add_argument("--num_beams", type=int, default=1, help="Beam width for beam search generation")
    parser.add_argument("--include_rubric", action="store_true", help="Include rubric in prompt instruction")
    parser.add_argument("--max_gen_tokens", type=int, default=200)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--grad_accum_steps", type=int, default=4)
    parser.add_argument("--mini_batch_size", type=int, default=16, help="Mini-batch size for PPO training")
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--lr", type=float, default=3e-4, help="Learning rate")
    parser.add_argument("--wd", type=float, default=0.0, help="Weight decay")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="Max gradient norm")
    parser.add_argument("--wandb", action="store_true", help="Log performance to wandb")
    args = parser.parse_args()

    if args.wandb:
        wandb.init(
            project="feedback-gen",
            group="llm-gen",
            config=args
        )

    if args.sft:
        sft(args)
    elif args.ppo:
        ppo(args)
    elif args.dpo:
        dpo(args)
    elif args.generate:
        generate(args)

if __name__ == "__main__":
    main()
