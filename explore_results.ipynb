{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate GPT-4 Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from create_reward_dataset import LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"correct\"] + LABELS[1:]\n",
    "\n",
    "gpt4_evals = pd.read_csv(\"data/annotated/feedback_test_single_subset_annotated_bin_gpt-4.csv\")\n",
    "manual_evals = pd.read_csv(\"data/annotated/feedback_test_single_subset_annotated_bin_manual.csv\")\n",
    "gpt4_evals = gpt4_evals.iloc[:len(manual_evals)]\n",
    "\n",
    "for df in [gpt4_evals, manual_evals]:\n",
    "    df[\"correct\"] = 1 - df[\"incorrect\"]\n",
    "    df[\"reveal\"] = 1 - df[\"reveal\"]\n",
    "    df[\"score\"] = df[\"correct\"] * sum([df[label] for label in LABELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_labels = ~manual_evals[\"incorrect\"].isna()\n",
    "gpt4_evals = gpt4_evals.loc[has_labels]\n",
    "manual_evals = manual_evals.loc[has_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff\n",
      "correct 18\n",
      "reveal 3\n",
      "suggestions 20\n",
      "misconceptions 25\n",
      "positive 14\n",
      "\n",
      "Only Diff\n",
      "correct 3\n",
      "reveal 1\n",
      "suggestions 7\n",
      "misconceptions 5\n",
      "positive 7\n",
      "\n",
      "Both True\n",
      "correct 39\n",
      "reveal 69\n",
      "suggestions 26\n",
      "misconceptions 28\n",
      "positive 7\n",
      "\n",
      "Both False\n",
      "correct 23\n",
      "reveal 8\n",
      "suggestions 34\n",
      "misconceptions 27\n",
      "positive 59\n"
     ]
    }
   ],
   "source": [
    "print(\"Diff\")\n",
    "label_to_diff = {label: gpt4_evals[label] != manual_evals[label] for label in labels}\n",
    "for label in label_to_diff:\n",
    "    print(label, label_to_diff[label].sum())\n",
    "\n",
    "print(\"\\nOnly Diff\")\n",
    "label_to_only_diff = {}\n",
    "for inc_label in labels:\n",
    "    mask = np.ones((len(manual_evals),), dtype=bool)\n",
    "    for label in labels:\n",
    "        if label == inc_label:\n",
    "            mask = mask & (gpt4_evals[label] != manual_evals[label])\n",
    "        else:\n",
    "            mask = mask & (gpt4_evals[label] == manual_evals[label])\n",
    "    label_to_only_diff[inc_label] = mask\n",
    "for label in label_to_only_diff:\n",
    "    print(label, label_to_only_diff[label].sum())\n",
    "\n",
    "print(\"\\nBoth True\")\n",
    "label_to_both_true = {label: gpt4_evals[label].astype(bool) & manual_evals[label].astype(bool) for label in labels}\n",
    "for label in label_to_both_true:\n",
    "    print(label, label_to_both_true[label].sum())\n",
    "\n",
    "print(\"\\nBoth False\")\n",
    "label_to_both_false = {label: ~gpt4_evals[label].astype(bool) & ~manual_evals[label].astype(bool) for label in labels}\n",
    "for label in label_to_both_false:\n",
    "    print(label, label_to_both_false[label].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_diffs_for_label(label, label_map):\n",
    "    print(label, label_map[label].sum())\n",
    "    print(\"\")\n",
    "    for (_, gpt4_sample), (_, manual_sample) in zip(\n",
    "        gpt4_evals[label_map[label]].iterrows(),\n",
    "        manual_evals[label_map[label]].iterrows()\n",
    "    ):\n",
    "        print(gpt4_sample[\"question\"])\n",
    "        print(gpt4_sample[\"correct_answer\"])\n",
    "        print(gpt4_sample[\"distractor\"])\n",
    "        print(gpt4_sample[\"feedback\"])\n",
    "        print(gpt4_sample[\"method\"])\n",
    "        print(\"GPT-4:\", gpt4_sample[label], \"Manual:\", manual_sample[label])\n",
    "        print(gpt4_sample[\"response\"])\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diffs_for_label(\"correct\", label_to_both_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = pd.read_csv(\"results/feedback_gen_results_meta-llama-Llama-2-7b-chat-hf_rubric_greedy_eval_llm.csv\")\n",
    "sft = pd.read_csv(\"results/feedback_gen_results_feedback-gen-sft-llama-chat-rubric_greedy_eval_llm.csv\")\n",
    "dpo_abl = pd.read_csv(\"results/feedback_gen_results_feedback-gen-dpo-llama-chat-rubric-nomm_greedy_eval_llm.csv\")\n",
    "dpo = pd.read_csv(\"results/feedback_gen_results_feedback-gen-dpo-llama-chat-rubric_greedy_eval_llm.csv\")\n",
    "human = pd.read_csv(\"results/eedi_expanded_test_eval_llm.csv\")\n",
    "gpt4 = pd.read_csv(\"results/feedback_test_zs_gpt-4_sol_rubric_eval_llm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desirable_idxs = (zs[\"score\"] < dpo[\"score\"]) & (sft[\"score\"] < dpo[\"score\"]) & (dpo_abl[\"score\"] <= dpo[\"score\"]) & (dpo[\"score\"] == 1)\n",
    "# desirable_idxs = (zs[\"score\"] < dpo_abl[\"score\"]) & (sft[\"score\"] < dpo_abl[\"score\"]) & (dpo_abl[\"score\"] == 1) & (human[\"score\"] == 1)\n",
    "# desirable_idxs = (zs[\"score\"] < dpo[\"score\"]) & (sft[\"score\"] < dpo[\"score\"]) & (dpo[\"score\"] == 1) & (human[\"score\"] == 1)\n",
    "print(desirable_idxs.sum())\n",
    "print(desirable_idxs.to_numpy().nonzero())\n",
    "for i in desirable_idxs.to_numpy().nonzero()[0]:\n",
    "    print(i)\n",
    "    print(human.iloc[i][\"question\"])\n",
    "    print(human.iloc[i][\"correct_answer\"])\n",
    "    print(human.iloc[i][\"distractor\"])\n",
    "    print(f\"Human ({human.iloc[i]['score']}):\", human.iloc[i][\"feedback\"])\n",
    "    print(f\"DPO ({dpo.iloc[i]['score']}):\", dpo.iloc[i][\"feedback\"])\n",
    "    print(f\"DPO abl ({dpo_abl.iloc[i]['score']}):\", dpo_abl.iloc[i][\"feedback\"])\n",
    "    print(f\"SFT ({sft.iloc[i]['score']}):\", sft.iloc[i][\"feedback\"])\n",
    "    print(f\"ZS ({zs.iloc[i]['score']}):\", zs.iloc[i][\"feedback\"])\n",
    "    print(f\"GPT-4 ({gpt4.iloc[i]['score']}):\", gpt4.iloc[i][\"feedback\"])\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
